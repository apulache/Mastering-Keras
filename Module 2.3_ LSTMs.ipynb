{"cells":[{"cell_type":"markdown","metadata":{"id":"DSDYSPCLIpsR","colab_type":"text"},"source":["## Module 2.3: Working with LSTMs in Keras (A Review)\n","\n","We turn to implementing a type of recurrent neural network know as LSTM in the Keras functional API. In this module we will pay attention to:\n","\n","1. Using the Keras functional API for defining models.\n","2. Mounting your Google drive to your Colab environment for file interface.\n","3. Generating synthetic data from a LSTM and sequence seed.\n","\n","Those students who are comfortable with all these matters might consider skipping ahead.\n","\n","Note that we will not spend time tuning hyper-parameters: The purpose is to show how different techniques can be implemented in Keras, not to solve particular data science problems as optimally as possible. Obviously, most techniques include hyper-parameters that need to be tuned for optimal performance."]},{"cell_type":"markdown","metadata":{"id":"TK1JgxD3LD1U","colab_type":"text"},"source":["First we import required libraries."]},{"cell_type":"code","metadata":{"id":"btVPO4ETIEfL","colab_type":"code","outputId":"25a62dae-91b6-4b76-b83d-53382e4404d4","executionInfo":{"status":"ok","timestamp":1576672451562,"user_tz":-60,"elapsed":2066,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":98},"tags":[]},"source":["import sys\n","import numpy\n","\n","# from google.colab import drive\n","\n","from keras.models import Sequential\n","from keras import Model\n","from keras.optimizers import Adadelta\n","from keras.layers import Dense,Dropout,LSTM,Input\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sGCD1zv5Low8","colab_type":"text"},"source":["We will have a little fun and try to teach a neural network to write like Lewis Carroll, the author of Alice in Wonderland.\n","\n","Note, though, that the same technique can be used to model any sequential system, and generate simulations from seeds for such a system. Here the sequence are the characters written by Carroll during Alice in Wonderland, but it could be, for example, an industrial system that evolves in time. In that case, when we generate simulations of the system based on current and recent conditions we simulate the expected evolution of the system - something of great value!"]},{"cell_type":"markdown","metadata":{"id":"sfYWdsSGQJlQ","colab_type":"text"},"source":["We will use the [Project Gutenburg text file of Alice in Wonderland](https://www.gutenberg.org/files/11/11.txt). But we need to get the file into our colab environment and this takes some work.\n","\n","First, you need to place the file in your google drive. We will assume that you will place it in a folder called \"Mastering Keras Datasets\", and that you rename it \"Alice.txt\". If you don't, you will need to the file path used in the code.\n","\n","Once you have done that, you will need to mount your google drive in Colab. Run the following code and complete the required authorizations.\n","\n","Note that you will need to mount your drive every time you use code from this tutorial."]},{"cell_type":"code","metadata":{"id":"IlKpjJfKcksv","colab_type":"code","outputId":"e57bb726-7439-455b-816d-4c1f442edb0e","executionInfo":{"status":"ok","timestamp":1571045131919,"user_tz":-120,"elapsed":45875,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["# Note: You will need to mount your drive every time you \n","# run code in this tutorial.\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oV3vUZryMUjT","colab_type":"text"},"source":["Now we can load the file using code and prepare the data. We want to work with sequences of 100 characters as input data, and our target will be the next (101st) character.\n","\n","To keep things simple, we will ignore upper/lower case character distinctions, and cast all alphabetical characters to lower case. To allow our model to work with these characters, we will encode them as integers. We will then normalize them to real numbers between 0 and 1 and add a dimension (we are working with a system with a single feature). Finally we will one-hot encode the target character (see previous module for discussion of one-hot encoding). This is not the only way to handle the data, but it is a simple one.\n","\n","We will also return the unnormalized and non-reshaped X data, the number of characters found and an integer coding to character dictionary, all for use later.\n"]},{"cell_type":"code","metadata":{"id":"-RXtz4LwLHoY","colab_type":"code","colab":{}},"source":["def load_alice (\n","    rawTextFile=\"DataSets/Alice.txt\"   \n","    ):\n","    # load ascii text and covert to lowercase\n","    raw_text = open(rawTextFile, encoding='utf-8').read()\n","    raw_text = raw_text.lower()\n","    # create mapping of unique chars to integers\n","    chars = sorted(list(set(raw_text)))\n","    char_to_int = dict((c, i) for i, c in enumerate(chars))\n","    int_to_char = dict((i, c) for i, c in enumerate(chars))\n","    # summarize the loaded data\n","    n_chars = len(raw_text)\n","    n_vocab = len(chars)\n","    print (\"Total Characters: \", n_chars)\n","    print (\"Total Vocab: \", n_vocab)\n","    # prepare the dataset of input to output pairs encoded as integers\n","    seq_length = 100\n","    dataX = []\n","    dataY = []\n","    for i in range(0, n_chars - seq_length, 1):\n","    \tseq_in = raw_text[i:i + seq_length]\n","    \tseq_out = raw_text[i + seq_length]\n","    \tdataX.append([char_to_int[char] for char in seq_in])\n","    \tdataY.append(char_to_int[seq_out])\n","    n_patterns = len(dataX)\n","    print (\"Total Patterns: \", n_patterns)\n","    # reshape X to be [samples, time steps, features]\n","    X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","    # normalize\n","    X = X / float(n_vocab)\n","    # one hot encode the output variable\n","    Y = np_utils.to_categorical(dataY)\n","    return X,Y,dataX,n_vocab,int_to_char"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LikRHGnkM8r-","colab_type":"text"},"source":["Now lets load the data. X and Y are the input and target label datasets we will use in training. X_ is the un-reshaped X data for use later."]},{"cell_type":"code","metadata":{"id":"kXdKny5NM_RT","colab_type":"code","outputId":"25e0e0d9-9daf-4ab2-9fd2-8cd256f775c1","executionInfo":{"status":"ok","timestamp":1571045177877,"user_tz":-120,"elapsed":3721,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":69},"tags":[]},"source":["X,Y,X_,n_vocab,int_to_char = load_alice()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHOixFxYdl1H","colab_type":"text"},"source":["You can play around below to look at the shape of the resulting X and Y arrays, as well as their contents. But they are no longer understandable character strings."]},{"cell_type":"code","metadata":{"id":"X2fL5Qbjdk1Z","colab_type":"code","colab":{}},"source":["# Play around here to look at data characteristics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3Wk5Dvzexjs","colab_type":"text"},"source":["Now we define our LSTM using the Keras function API. We are going to make use of LSTM layers, and add a dropout layer for regularization.\n","\n","We will pass the data to the model defining function so that we can read input and output dimensions of it, rather than hard coding them.\n","\n","For comparison, a second version of the function is included showing how to use the sequential approach."]},{"cell_type":"code","metadata":{"id":"YpQik6VXIdTl","colab_type":"code","colab":{}},"source":["def get_model (X,Y):\n","    # define the LSTM model\n","    inputs=Input(shape=(X.shape[1],X.shape[2]),name=\"Input\")\n","    lstm1=LSTM(256, input_shape=(100,1),return_sequences=True)(inputs)\n","    drop1=Dropout(0.2)(lstm1)\n","    lstm2=LSTM(256)(drop1)\n","    drop2=Dropout(0.2)(lstm2)\n","    outputs=Dense(Y.shape[1], activation='softmax')(drop2)\n","    model=Model(inputs=inputs,outputs=outputs)\n","    return model\n","\n","def get_model_sequential (X,Y):\n","    # define the LSTM model\n","    model = Sequential()\n","    model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n","    model.add(Dropout(0.2))\n","    model.add(LSTM(256))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(Y.shape[1], activation='softmax'))\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25ea1_kwhKSL","colab_type":"text"},"source":["We get our model."]},{"cell_type":"code","metadata":{"id":"yHO2WNkIhPz4","colab_type":"code","colab":{}},"source":["model=get_model(X,Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B8AlPqyChSLp","colab_type":"text"},"source":["Now we will define an optimizer and compile it. If you are unfamiliar with the different types of optimizers available in keras, I suggest you read the keras documentation [here](https://keras.io/optimizers/) and play around training the model with different alternatives."]},{"cell_type":"code","metadata":{"id":"D4QI98iFhdRn","colab_type":"code","colab":{}},"source":["opt=Adadelta()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNvQlYwUhhps","colab_type":"text"},"source":["And we compile our model with the optimizer ready for training. We use categorical crossentropy as our loss function as this is a good default choice for working with a multi-class categorical target variable (i.e. the next character labels)."]},{"cell_type":"code","metadata":{"id":"-gBX1zHbh8T_","colab_type":"code","colab":{}},"source":["model.compile(optimizer=opt,\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WxHPgLnEkZdn","colab_type":"text"},"source":["Now we will make a function to fit the model. We will not do this very professionally (it is just a fun project), and so will not use any validation data. Rather, we will just run the training for a number of epoches - by default 20, though you can change this.\n","\n","We will, though, use a ModelCheckpoint callback to save the best performing weights and load these into the model and the conclusion of the training. Note that training performance should normally improve with more epoches, so this is unlikely to improve performance. What we really want is to be able to load the best weights without having to redo the training process (see below)\n","\n","If you want to, you are encouraged to alter the code in this tutorial to work with a training and validation set, and adjust the fit function below to incorporate an EarlyStopping callback based on performance on the validation data.\n","\n","We have two one LSTM layer, we are dealing with sequences of length 100. So if we 'unroll' it, we have a network of 200 LSTM layers. And inside these layers are infact multiple internal layers setting up the LSTM architecture! So this is actually a pretty big network, and training will take some time (about 200 hours on the free Colab environment for 200 epochs). This is probably too much to conveniently run yourself.\n","\n","Here we have an example of how we could train it on Colab. Colab will eventually time out. The best thing to do is to save our weights file to our google drive, so we can load it at leisure later and resume training. This is what we will do. Remember that if you didn't use the default name for your folder in your google drive you should change the path string in the code.\n","\n","In real life, you will also often want to save the state of the optimizer (so that it keeps its current learning rate, etc). You can do this by accessing and saving model.optimizer.get_state(). It is left as an exercise to implement this.\n","\n","*It is not expected that you train the network using this function - see below to load trained weights from your google drive.*"]},{"cell_type":"code","metadata":{"id":"2W1MXN18kZ3N","colab_type":"code","colab":{}},"source":["def fit_model (model,X,Y,epochs=100):\n","    # define the checkpoint callback\n","    filepath=\"DataSets/alice_best_weights.hdf5\" \n","    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n","                                 save_best_only=True, mode='min')\n","    callbacks_list = [checkpoint]\n","    # fit the model\n","    model.fit(X, Y, epochs=epochs, batch_size=128, callbacks=callbacks_list)\n","    # load the best weights\n","    model.load_weights(filename)\n","    # return the final model\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ek8_BWfrh8fm","colab_type":"text"},"source":["We would then fit (train) the model by calling the above function.\n","\n","*It is not expected that you train the network using this function - see below to load trained weights from your google drive.*"]},{"cell_type":"code","metadata":{"id":"HzsCLeYthGK-","colab_type":"code","outputId":"4b13c24e-78c3-4d98-ac83-1ca72b330017","executionInfo":{"status":"error","timestamp":1570711632825,"user_tz":-120,"elapsed":3247167,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":504},"tags":[]},"source":["model=fit_model(model,X,Y,100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clAYq_A81Cob","colab_type":"text"},"source":["Here we will load saved weights. You can use the \"alice_best_weights.hdf5\" file that comes with the course - just place it in the same folder as the \"alice.txt\" file in your google drive. This file has been trained for 200 epoches, and gets a loss around 1.16.\n","\n","If you train the network yourself, the best weights will be saved as \"alice_best_weights.hdf5\" in the same location as above. You can therefore use the same code in both cases.\n","\n","In all cases remember to change the filepath if you are not using the default folder name.\n","\n","If you are resuming this tutorial here in a new session, you should re-mount your Google drive using the earlier code, re-load the data, and then run this code block to load the weights into a new model. \n","\n","If you want to train the model further, you will need to compile it with an optimizer."]},{"cell_type":"code","metadata":{"id":"A7bvAwsEfzlo","colab_type":"code","outputId":"2d48bd39-c481-4caa-e8dd-a49b2b2903cf","executionInfo":{"status":"ok","timestamp":1571043657639,"user_tz":-120,"elapsed":1835,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":435}},"source":["model=get_model(X,Y)\n","filepath=\"DataSets/alice_best_weights.hdf5\"\n","model.load_weights(filepath)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1x9CiNJ1tjY","colab_type":"text"},"source":["Now we can see if our network has mastered the art of writing like Lewis Carroll! Let's write a function to let us see, and then call it."]},{"cell_type":"code","metadata":{"id":"qC2dzmdpITMG","colab_type":"code","colab":{}},"source":["def write_like_Lewis_Carroll(model,X_,n_vocab,int_to_char):\n","  # pick a random seed...\n","  start = numpy.random.randint(0, len(X_)-1)\n","  # ... in order to decide which X datum to use to start\n","  pattern = X_[start]\n","\n","  print (\"Seed:\")\n","  print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","  # generate characters\n","  for i in range(1000):\n","    # We transform the integer mapping of the characters to\n","    # real numbers suitable for input into our model.\n","    x = numpy.reshape(pattern, (1, len(pattern), 1))\n","    x = x/float(n_vocab)\n","    # We use the model to estimate the probability distribution for\n","    # the next character\n","    prediction = model.predict(x, verbose=0)\n","    # We choose as the next character whichever the model thinks is most likely\n","    index = numpy.argmax(prediction)\n","    result = int_to_char[index]\n","    seq_in = [int_to_char[value] for value in pattern]\n","    sys.stdout.write(result)\n","    # We add the integer to our pattern... \n","    pattern.append(index)\n","    # ... and drop the earliest integer from our pattern.\n","    pattern = pattern[1:len(pattern)]\n","  print (\"\\nDone.\")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"aG-PGS0vISCB","colab_type":"code","outputId":"8896db20-cfed-4a7a-bbfc-c539508b3446","executionInfo":{"status":"ok","timestamp":1571043754022,"user_tz":-120,"elapsed":24907,"user":{"displayName":"Michael Ashcroft","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAaRQqpOuFHR3D_ZulW6qlXPomIq5vZ-wR4ZuIm=s64","userId":"16725792548700883920"}},"colab":{"base_uri":"https://localhost:8080/","height":418},"tags":[]},"source":["write_like_Lewis_Carroll(model,X_,n_vocab,int_to_char)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"Seed:\n\" h, you're sure to do that,' said the cat, 'if you only walk long\nenough.'\n\nalice felt that this coul \"\nd not ee to mearn the tea,aut itmakns tobe a very wankeof the soallesthat shey dould not mone booear as the shing of the court, and she whole party and said to the garden, andthen said to the harden, and then she was not atall crmeup tourdd as her feadof the morent to be then thewas the rabbitsay off and louthbd btthe court, and she thought it out the foomah out of the cankse, and hed down the canled her oef whrhout tas the sook andrurtirefor toeer onthe table, thewas thefoon thepoor of the tore, andthen said to the corroa said anothermust bethe fatt and all the sopa taid in a mittle wasto be aly of them. and the white rabbit retrengd at her fout tere time, and said to herself, and she thought the pueentaid soiethenly sp the bat and apail of the coor, and the whiterabbit retuie and cown to the tabbit say the firtance of the sopm,\n'and you douldn't have wound be gite,' said the caterpillar.\n\n'imever said the world you couldn't gave was onthing again!' saidthe\nDone.\n"}]},{"cell_type":"markdown","metadata":{"id":"BG_f2DsQgW5_","colab_type":"text"},"source":["If you run the above a few times, you will see that we have had some success - though we are still a long way from a good Alice in Wonderland simulator!\n","\n","Here is an extract from one simulation I ran:\n","\n","*'i should hit tere things,' said the caterpillar.*\n","\n","*'well, perhaps you may bean the same siings tuertion,' the duchess said to the gryphon.*\n","\n","*'what i cen the thing,' said the caterpillar.*\n","\n","*'well, perhaps you may bean the same siings tuertion,' the mock turtle seplied,*\n","\n","*'that i man the mice,' said the caterpillar.*\n","\n","We have got to the point of basic sentence structure, quotations for speech, plausible characters given the context, etc. There remains misspellings, and occasional punctuation errors, and other issues. (And this was a good selection.) \n","\n","In fact, you should be able to do much better. Trying with 500 time points (predicting the 501st character from the preceeding 500) and using a three layer LSTM will lead to major improvements. So would using more training data (multiple Lewis Carole books). You can see the performance achieved on a Shakespeare simulator [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n","\n","If you have time, consider it an exercise to try to improve this implementation to that level - but be warned, the suggested changes would lead to training time being about 7 times longer for the same number of epochs, and of course more epoches would be required as it would be a more complex model. Since it would have taken 100+ hours on the Colab environment (which disconnects after a time limit) this is really only an exercise for those with access to a powerful local environment. "]}],"metadata":{"colab":{"name":"Module 2.3: LSTMs.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}